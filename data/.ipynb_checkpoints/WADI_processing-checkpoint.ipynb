{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53aed9fd-c78c-4d1c-96e9-9d29538617cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.chdir(r'D:\\lym本科\\科研相关\\华为项目资料\\2025-1-23\\Encoder_PELT')   # set as your own working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13bd3167-96d7-404f-a435-7cda83562b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_windows(data, window_size):\n",
    "    windows = []\n",
    "    for i in range(len(data) - window_size + 1):\n",
    "        window = data[i:i+window_size,:]\n",
    "        windows.append(window)\n",
    "    return np.array(windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5c854e8-e18d-4c44-b707-1e15dc42cea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Missing Values Analysis ===\n",
      "Total samples: 784571\n",
      "\n",
      "Missing Values of Col:\n",
      "                          Missing Values  Missing Percentile\n",
      "Row                                    0            0.000000\n",
      "Date                                   0            0.000000\n",
      "Time                                   0            0.000000\n",
      "1_AIT_001_PV                           0            0.000000\n",
      "1_AIT_002_PV                          12            0.001529\n",
      "...                                  ...                 ...\n",
      "3_P_003_STATUS                         0            0.000000\n",
      "3_P_004_STATUS                         0            0.000000\n",
      "LEAK_DIFF_PRESSURE                     0            0.000000\n",
      "PLANT_START_STOP_LOG                   0            0.000000\n",
      "TOTAL_CONS_REQUIRED_FLOW               0            0.000000\n",
      "\n",
      "[130 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f'./data/WADI/WADI_14days_new.csv')   # set as your own working directory\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_data = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Missing Percentile': missing_percent\n",
    "})\n",
    "\n",
    "print(\"=== Missing Values Analysis ===\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(\"\\nMissing Values of Col:\")\n",
    "print(missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fb3f519-da5a-422c-a0fa-9c9852826f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Before Processing ===\n",
      "Original data shape (rows, cols): (784571, 130)\n",
      "Original row count: 784571\n",
      "Original column count: 130\n",
      "\n",
      "=== Intermediate Processing (Dropping High-Missing Columns) ===\n",
      "Number of columns dropped: 4\n",
      "Dropped columns (missing >50%):\n",
      "- 2_P_002_STATUS (Missing: 100.0%)\n",
      "- 2_P_001_STATUS (Missing: 100.0%)\n",
      "- 2_LS_001_AL (Missing: 100.0%)\n",
      "- 2_LS_002_AL (Missing: 100.0%)\n",
      "Intermediate data shape (rows, cols): (784571, 126)\n",
      "\n",
      "=== After Final Processing ===\n",
      "Final data shape (rows, cols): (784537, 126)\n",
      "Number of rows dropped: 34\n",
      "\n",
      "Dropped row indices (total 34 rows):\n",
      "[61703, 61704, 61705, 61706, 61707, 61708, 61709, 61710, 61711, 61712]\n"
     ]
    }
   ],
   "source": [
    "threshold_percent = 50\n",
    "threshold_count = len(df) * threshold_percent / 100\n",
    "\n",
    "cols_before = df.columns.tolist()\n",
    "df_interim = df.dropna(axis=1, thresh=len(df) - threshold_count)\n",
    "cols_after = df_interim.columns.tolist()\n",
    "dropped_columns = set(cols_before) - set(cols_after)\n",
    "\n",
    "df_cleaned = df_interim.dropna(axis=0)\n",
    "\n",
    "print(\"=== Before Processing ===\")\n",
    "print(f\"Original data shape (rows, cols): {df.shape}\")\n",
    "print(f\"Original row count: {len(df)}\")\n",
    "print(f\"Original column count: {len(df.columns)}\")\n",
    "\n",
    "print(\"\\n=== Intermediate Processing (Dropping High-Missing Columns) ===\")\n",
    "print(f\"Number of columns dropped: {len(dropped_columns)}\")\n",
    "if dropped_columns:\n",
    "    print(\"Dropped columns (missing >{}%):\".format(threshold_percent))\n",
    "    for col in dropped_columns:\n",
    "        missing_percent = df[col].isnull().sum() / len(df) * 100\n",
    "        print(f\"- {col} (Missing: {missing_percent:.1f}%)\")\n",
    "print(f\"Intermediate data shape (rows, cols): {df_interim.shape}\")\n",
    "\n",
    "print(\"\\n=== After Final Processing ===\")\n",
    "print(f\"Final data shape (rows, cols): {df_cleaned.shape}\")\n",
    "print(f\"Number of rows dropped: {len(df_interim) - len(df_cleaned)}\")\n",
    "\n",
    "dropped_rows = set(df_interim.index) - set(df_cleaned.index)\n",
    "if len(dropped_rows) > 0:\n",
    "    print(f\"\\nDropped row indices (total {len(dropped_rows)} rows):\")\n",
    "    print(sorted(dropped_rows)[:10])\n",
    "else:\n",
    "    print(\"\\nNo rows dropped (no missing values in data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8fb1141-24f6-46ad-9ed6-b7f230d8fb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 55199\n",
      "\n",
      "First 5 Rows：\n",
      "        1_AIT_001_PV  1_AIT_002_PV  1_AIT_003_PV  1_AIT_004_PV  1_AIT_005_PV  \\\n",
      "280800       180.283      0.589481       11.5514       513.361      0.331398   \n",
      "280801       180.317      0.607475       11.5530       513.361      0.331556   \n",
      "280802       180.317      0.607475       11.5530       513.361      0.331556   \n",
      "280803       180.317      0.607475       11.5530       513.361      0.331556   \n",
      "280804       180.317      0.607475       11.5530       513.361      0.331556   \n",
      "\n",
      "        1_FIT_001_PV  1_LS_001_AL  1_LS_002_AL  1_LT_001_PV  1_MV_001_STATUS  \\\n",
      "280800      0.001127            0            0      64.9483                1   \n",
      "280801      0.001122            0            0      64.9450                1   \n",
      "280802      0.001122            0            0      64.9450                1   \n",
      "280803      0.001122            0            0      64.9450                1   \n",
      "280804      0.001122            0            0      64.9450                1   \n",
      "\n",
      "        ...  3_MV_001_STATUS  3_MV_002_STATUS  3_MV_003_STATUS  \\\n",
      "280800  ...                1                1                1   \n",
      "280801  ...                1                1                1   \n",
      "280802  ...                1                1                1   \n",
      "280803  ...                1                1                1   \n",
      "280804  ...                1                1                1   \n",
      "\n",
      "        3_P_001_STATUS  3_P_002_STATUS  3_P_003_STATUS  3_P_004_STATUS  \\\n",
      "280800               1               1               1               1   \n",
      "280801               1               1               1               1   \n",
      "280802               1               1               1               1   \n",
      "280803               1               1               1               1   \n",
      "280804               1               1               1               1   \n",
      "\n",
      "        LEAK_DIFF_PRESSURE  PLANT_START_STOP_LOG  TOTAL_CONS_REQUIRED_FLOW  \n",
      "280800             59.9978                     1                      0.11  \n",
      "280801             59.9157                     1                      0.11  \n",
      "280802             59.9157                     1                      0.11  \n",
      "280803             59.9157                     1                      0.11  \n",
      "280804             59.9157                     1                      0.11  \n",
      "\n",
      "[5 rows x 123 columns]\n",
      "train_window: (55190, 1230)\n"
     ]
    }
   ],
   "source": [
    "filtered_data = df_cleaned.loc[df['Date'] == '9/29/2017'].drop(columns=['Row', 'Date', 'Time'])\n",
    "print(f\"Samples: {len(filtered_data)}\")\n",
    "print(\"\\nFirst 5 Rows：\")\n",
    "print(filtered_data.head())\n",
    "\n",
    "window_size = 10\n",
    "filtered_data_array = filtered_data.to_numpy(dtype='float32')\n",
    "train_window = create_windows(filtered_data_array, window_size)\n",
    "train_window = train_window.reshape(train_window.shape[0],-1)\n",
    "print(\"train_window:\",train_window.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe2934c6-95fb-4ad1-8f89-cff1ac6e2932",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\QH\\AppData\\Local\\Temp\\ipykernel_34336\\1587462825.py:1: DtypeWarning: Columns (0,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(f'./data/WADI/WADI_attackdataLABLE.csv')   # set as your own working directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Missing Values Analysis ===\n",
      "Total Samples: 172804\n",
      "\n",
      "Missing Values of Col:\n",
      "     Missing Values  Missing Percentile\n",
      "0                 2            0.001157\n",
      "1                 2            0.001157\n",
      "2                 2            0.001157\n",
      "3                 2            0.001157\n",
      "4                 2            0.001157\n",
      "..              ...                 ...\n",
      "126               2            0.001157\n",
      "127               2            0.001157\n",
      "128               2            0.001157\n",
      "129               2            0.001157\n",
      "130               0            0.000000\n",
      "\n",
      "[131 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f'./data/WADI/WADI_attackdataLABLE.csv')   # set as your own working directory\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_data = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Missing Percentile': missing_percent\n",
    "})\n",
    "\n",
    "print(\"=== Missing Values Analysis ===\")\n",
    "print(f\"Total Samples: {len(df)}\")\n",
    "print(\"\\nMissing Values of Col:\")\n",
    "print(missing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4323d93e-c004-48e9-b86f-f11ad70d6120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Before Processing ===\n",
      "Original data shape (rows, columns): (172804, 131)\n",
      "Original row count: 172804\n",
      "Original column count: 131\n",
      "\n",
      "=== Intermediate Processing (Removing High-Missing Columns) ===\n",
      "Number of columns removed: 4\n",
      "Removed columns (missing values >50%):\n",
      "- 87 (Missing values: 100.0%)\n",
      "- 51 (Missing values: 100.0%)\n",
      "- 86 (Missing values: 100.0%)\n",
      "- 50 (Missing values: 100.0%)\n",
      "Intermediate data shape (rows, columns): (172804, 127)\n",
      "\n",
      "=== After Final Processing ===\n",
      "Final data shape (rows, columns): (172802, 127)\n",
      "Number of rows removed: 2\n",
      "\n",
      "Removed row indices (Total 2 rows):\n",
      "[172802, 172803]\n"
     ]
    }
   ],
   "source": [
    "threshold_percent = 50\n",
    "threshold_count = len(df) * threshold_percent / 100\n",
    "\n",
    "cols_before = df.columns.tolist()\n",
    "df_interim = df.dropna(axis=1, thresh=len(df) - threshold_count)\n",
    "cols_after = df_interim.columns.tolist()\n",
    "dropped_columns = set(cols_before) - set(cols_after)\n",
    "\n",
    "df_cleaned = df_interim.dropna(axis=0)\n",
    "\n",
    "print(\"=== Before Processing ===\")\n",
    "print(f\"Original data shape (rows, columns): {df.shape}\")\n",
    "print(f\"Original row count: {len(df)}\")\n",
    "print(f\"Original column count: {len(df.columns)}\")\n",
    "\n",
    "print(\"\\n=== Intermediate Processing (Removing High-Missing Columns) ===\")\n",
    "print(f\"Number of columns removed: {len(dropped_columns)}\")\n",
    "if dropped_columns:\n",
    "    print(\"Removed columns (missing values >{}%):\".format(threshold_percent))\n",
    "    for col in dropped_columns:\n",
    "        missing_percent = df[col].isnull().sum() / len(df) * 100\n",
    "        print(f\"- {col} (Missing values: {missing_percent:.1f}%)\")\n",
    "print(f\"Intermediate data shape (rows, columns): {df_interim.shape}\")\n",
    "\n",
    "print(\"\\n=== After Final Processing ===\")\n",
    "print(f\"Final data shape (rows, columns): {df_cleaned.shape}\")\n",
    "print(f\"Number of rows removed: {len(df_interim) - len(df_cleaned)}\")\n",
    "\n",
    "dropped_rows = set(df_interim.index) - set(df_cleaned.index)\n",
    "if len(dropped_rows) > 0:\n",
    "    print(f\"\\nRemoved row indices (Total {len(dropped_rows)} rows):\")\n",
    "    print(sorted(dropped_rows)[:10])\n",
    "else:\n",
    "    print(\"\\nNo rows removed (No missing values in data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11acf4ca-12ef-4ab8-b378-b6abc36054b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_window: (172792, 1230)\n",
      "test_label: (172792,)\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = df_cleaned.drop(index=0)\n",
    "test_feature = df_cleaned.drop(columns='130')\n",
    "y = df_cleaned['130']\n",
    "y_transformed = y.replace({1: 0, -1: 1})\n",
    "\n",
    "filtered_data = test_feature.drop(columns=['0', '1', '2'])\n",
    "test_feature_array = filtered_data.to_numpy(dtype='float32')\n",
    "test_window = create_windows(test_feature_array, window_size)\n",
    "test_window = test_window.reshape(test_window.shape[0],-1)\n",
    "test_label = y_transformed.to_numpy(dtype='int')\n",
    "test_label = test_label[window_size-1:]\n",
    "print(\"test_window:\",test_window.shape)\n",
    "print(\"test_label:\",test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ad93a68-188b-4604-a59b-9000b54cabcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'./data/WADI/train_{window_size}_v4.npy', train_window)\n",
    "np.save(f'./data/WADI/test_{window_size}_v4.npy', test_window)\n",
    "np.save(f'./data/WADI/test_label_{window_size}_v4.npy', test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ae201d-3de2-4bbf-9c3d-5a95752cd32e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
